{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fee9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Sequence\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from flax import linen as nn\n",
    "from jax import jvp, value_and_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2db3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPPINN(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, a, b, c, d, e):\n",
    "        inputs, outputs = [a, b, c, d, e], []\n",
    "        init = nn.initializers.xavier_uniform()\n",
    "        for X in inputs:\n",
    "            for fs in self.features[:-1]:\n",
    "                X = nn.Dense(fs, kernel_init=init)(X)\n",
    "                X = nn.activation.tanh(X)\n",
    "            X = nn.Dense(self.features[-1], kernel_init=init)(X)\n",
    "\n",
    "            outputs += [jnp.transpose(X, (1, 0))]\n",
    "\n",
    "        return jnp.einsum(\n",
    "            \"ra,rb,rc,rd,re->abcde\",\n",
    "            outputs[0],\n",
    "            outputs[1],\n",
    "            outputs[2],\n",
    "            outputs[3],\n",
    "            outputs[4],\n",
    "        )\n",
    "\n",
    "\n",
    "class TTPINN(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, a, b, c, d, e):\n",
    "        inputs, outputs = [a, b, c, d, e], []\n",
    "        init = nn.initializers.xavier_uniform()\n",
    "        for i, X in enumerate(inputs):\n",
    "            for fs in self.features[:-1]:\n",
    "                X = nn.Dense(fs, kernel_init=init)(X)\n",
    "                X = nn.activation.tanh(X)\n",
    "            if i != 0 and i != 4:\n",
    "                X = nn.DenseGeneral((self.features[-1], self.features[-1]), kernel_init=init)(X)\n",
    "            else:\n",
    "                X = nn.Dense(self.features[-1], kernel_init=init)(X)\n",
    "            outputs += [X]\n",
    "        return jnp.einsum(\n",
    "            \"a1,b12,c23,d34,e4->abcde\",\n",
    "            outputs[0],\n",
    "            outputs[1],\n",
    "            outputs[2],\n",
    "            outputs[3],\n",
    "            outputs[4],\n",
    "        )\n",
    "\n",
    "\n",
    "class TuckerPINN(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    def setup(self):\n",
    "        self.core = self.param(\n",
    "            \"core\",\n",
    "            nn.initializers.orthogonal(),\n",
    "            (\n",
    "                self.features[-1],\n",
    "                self.features[-1],\n",
    "                self.features[-1],\n",
    "                self.features[-1],\n",
    "                self.features[-1],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, a, b, c, d, e):\n",
    "        inputs, outputs = [a, b, c, d, e], []\n",
    "        init = nn.initializers.xavier_normal()\n",
    "        for X in inputs:\n",
    "            for fs in self.features[:-1]:\n",
    "                X = nn.Dense(fs, kernel_init=init)(X)\n",
    "                X = nn.activation.tanh(X)\n",
    "            X = nn.Dense(self.features[-1], kernel_init=init)(X)\n",
    "\n",
    "            outputs += [jnp.transpose(X, (1, 0))]\n",
    "        return jnp.einsum(\n",
    "            \"klmno,ka,lb,mc,nd,oe->abcde\",\n",
    "            self.core,\n",
    "            outputs[0],\n",
    "            outputs[1],\n",
    "            outputs[2],\n",
    "            outputs[3],\n",
    "            outputs[4],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed5b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp_fwdfwd(f, primals, tangents, return_primals=False):\n",
    "    # g = lambda primals: jvp(f, (primals,), tangents)[1]\n",
    "    def g(primals):\n",
    "        return jvp(f, (primals,), tangents)[1]\n",
    "\n",
    "    primals_out, tangents_out = jvp(g, primals, tangents)\n",
    "    if return_primals:\n",
    "        return primals_out, tangents_out\n",
    "\n",
    "    return tangents_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_poisson(apply_fn, *train_data):\n",
    "    def residual_loss(params, a, b, c, d, e, source):\n",
    "\n",
    "        # tangent vector dx/dx\n",
    "        # v_f = jnp.ones(f.shape)\n",
    "        v_a = jnp.ones(a.shape)\n",
    "        v_b = jnp.ones(b.shape)\n",
    "        v_c = jnp.ones(c.shape)\n",
    "        v_d = jnp.ones(d.shape)\n",
    "        v_e = jnp.ones(e.shape)\n",
    "\n",
    "        uaa = hvp_fwdfwd(lambda a: apply_fn(params, a, b, c, d, e), (a,), (v_a,))\n",
    "        ubb = hvp_fwdfwd(lambda b: apply_fn(params, a, b, c, d, e), (b,), (v_b,))\n",
    "        ucc = hvp_fwdfwd(lambda c: apply_fn(params, a, b, c, d, e), (c,), (v_c,))\n",
    "        udd = hvp_fwdfwd(lambda d: apply_fn(params, a, b, c, d, e), (d,), (v_d,))\n",
    "        uee = hvp_fwdfwd(lambda e: apply_fn(params, a, b, c, d, e), (e,), (v_e,))\n",
    "        # uff = hvp_fwdfwd(lambda t: apply_fn(params,a,b,c,d,e,f), (f,), (v_f,))\n",
    "        nabla_u = uaa + ubb + ucc + udd + uee\n",
    "        return jnp.mean((nabla_u + source) ** 2)\n",
    "\n",
    "    def boundary_loss(params, a, b, c, d, e, u):\n",
    "        loss = 0\n",
    "        for i in range(10):\n",
    "            loss += jnp.mean((apply_fn(params, a[i], b[i], c[i], d[i], e[i]) - u[i]) ** 2)\n",
    "            return loss\n",
    "\n",
    "    ac, bc, cc, dc, ec, source_term, ab, bb, cb, db, eb, ub = train_data\n",
    "    loss_fn = lambda params: residual_loss(params, ac, bc, cc, dc, ec, source_term) + boundary_loss(params, ab, bb, cb, db, eb, ub)\n",
    "    return loss_fn\n",
    "\n",
    "\n",
    "# optimizer step function\n",
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def update_model(optim, gradient, params, state):\n",
    "    updates, state = optim.update(gradient, state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f894c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_exact(a, b, c, d, e):\n",
    "    sol = 0\n",
    "    for i in [a, b, c, d, e]:\n",
    "        sol += jnp.sin((jnp.pi / 2) * i)\n",
    "    return sol\n",
    "\n",
    "\n",
    "def relative_l2(u, u_gt):\n",
    "    return jnp.linalg.norm(u - u_gt) / jnp.linalg.norm(u_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e5c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(nc, key):\n",
    "    keys = jax.random.split(key, 6)\n",
    "    ac = jax.random.uniform(keys[0], (nc,), minval=0.0, maxval=1.0)\n",
    "    bc = jax.random.uniform(keys[1], (nc,), minval=0.0, maxval=1.0)\n",
    "    cc = jax.random.uniform(keys[2], (nc,), minval=0.0, maxval=1.0)\n",
    "    dc = jax.random.uniform(keys[3], (nc,), minval=0.0, maxval=1.0)\n",
    "    ec = jax.random.uniform(keys[4], (nc,), minval=0.0, maxval=1.0)\n",
    "    # fc = jax.random.uniform(keys[5], (nc,), minval=0., maxval=1.)\n",
    "\n",
    "    acm, bcm, ccm, dcm, ecm = jnp.meshgrid(ac, bc, cc, dc, ec, indexing=\"ij\")\n",
    "    source_term = 0\n",
    "    for i in [acm, bcm, ccm, dcm, ecm]:\n",
    "        source_term = source_term + ((jnp.pi * jnp.pi / 4) * jnp.sin((jnp.pi / 2) * i))\n",
    "\n",
    "    ac = ac.reshape(-1, 1)\n",
    "    bc = bc.reshape(-1, 1)\n",
    "    cc = cc.reshape(-1, 1)\n",
    "    dc = dc.reshape(-1, 1)\n",
    "    ec = ec.reshape(-1, 1)\n",
    "\n",
    "    ab = [jnp.array([[0.0]]), jnp.array([[1.0]]), ac, ac, ac, ac, ac, ac, ac, ac]\n",
    "    bb = [bc, bc, jnp.array([[0.0]]), jnp.array([[1.0]]), bc, bc, bc, bc, bc, bc]\n",
    "    cb = [cc, cc, cc, cc, jnp.array([[0.0]]), jnp.array([[1.0]]), cc, cc, cc, cc]\n",
    "    db = [dc, dc, dc, dc, dc, dc, jnp.array([[0.0]]), jnp.array([[1.0]]), dc, dc]\n",
    "    eb = [ec, ec, ec, ec, ec, ec, ec, ec, jnp.array([[0.0]]), jnp.array([[1.0]])]\n",
    "\n",
    "    ub = []\n",
    "    for i in range(10):\n",
    "        abm, bbm, cbm, dbm, ebm = jnp.meshgrid(ab[i].ravel(), bb[i].ravel(), cb[i].ravel(), db[i].ravel(), eb[i].ravel(), indexing=\"ij\")\n",
    "        ub += [poisson_exact(abm, bbm, cbm, dbm, ebm)]\n",
    "\n",
    "    return ac, bc, cc, dc, ec, source_term, ab, bb, cb, db, eb, ub\n",
    "\n",
    "\n",
    "def test_generator(nc_test):\n",
    "    a = jnp.linspace(0, 1, nc_test)\n",
    "    b = jnp.linspace(0, 1, nc_test)\n",
    "    c = jnp.linspace(0, 1, nc_test)\n",
    "    d = jnp.linspace(0, 1, nc_test)\n",
    "    e = jnp.linspace(0, 1, nc_test)\n",
    "    # f = jnp.linspace(0,1,nc_test)\n",
    "\n",
    "    am, bm, cm, dm, em = jnp.meshgrid(a, b, c, d, e, indexing=\"ij\")\n",
    "\n",
    "    u_gt = poisson_exact(am, bm, cm, dm, em)\n",
    "\n",
    "    a = a.reshape(-1, 1)\n",
    "    b = b.reshape(-1, 1)\n",
    "    c = c.reshape(-1, 1)\n",
    "    d = d.reshape(-1, 1)\n",
    "    e = e.reshape(-1, 1)\n",
    "    # f = f.reshape(-1,1)\n",
    "\n",
    "    return a, b, c, d, e, am, bm, cm, dm, em, u_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0064839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(mode, NC, NI, NB, NC_TEST, SEED, LR, EPOCHS, N_LAYERS, FEATURES, LOG_ITER):\n",
    "    # force jax to use one device\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "    # random key\n",
    "    key = jax.random.PRNGKey(SEED)\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "\n",
    "    # feature sizes\n",
    "    feat_sizes = tuple(FEATURES for _ in range(N_LAYERS))\n",
    "\n",
    "    if mode == \"CPPINN\":\n",
    "        model = CPPINN(feat_sizes)\n",
    "    elif mode == \"TTPINN\":\n",
    "        model = TTPINN(feat_sizes)\n",
    "    elif mode == \"TuckerPINN\":\n",
    "        model = TuckerPINN(feat_sizes)\n",
    "\n",
    "    params = model.init(\n",
    "        subkey, jax.random.uniform(key, (NC, 1)), jax.random.uniform(key, (NC, 1)), jax.random.uniform(key, (NC, 1)), jax.random.uniform(key, (NC, 1)), jax.random.uniform(key, (NC, 1))\n",
    "    )\n",
    "    # optimizer\n",
    "    optim = optax.adam(LR)\n",
    "    state = optim.init(params)\n",
    "\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "    train_data = train_generator(NC, subkey)\n",
    "\n",
    "    a, b, c, d, e, am, bm, cm, dm, em, u_gt = test_generator(NC_TEST)\n",
    "    logger = []\n",
    "\n",
    "    apply_fn = jax.jit(model.apply)\n",
    "    loss_fn = loss_poisson(apply_fn, *train_data)\n",
    "\n",
    "    @jax.jit\n",
    "    def train_one_step(params, state):\n",
    "        # compute loss and gradient\n",
    "        loss, gradient = value_and_grad(loss_fn)(params)\n",
    "        # update state\n",
    "        params, state = update_model(optim, gradient, params, state)\n",
    "        return loss, params, state\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    pbar = tqdm.tqdm(total=EPOCHS)\n",
    "    error = np.nan\n",
    "\n",
    "    for iters in range(1, EPOCHS + 1):\n",
    "        # single run\n",
    "        loss, params, state = train_one_step(params, state)\n",
    "\n",
    "        if iters % LOG_ITER == 0 or iters == 1:\n",
    "            u = apply_fn(params, a, b, c, d, e)\n",
    "            error = relative_l2(u, u_gt)\n",
    "            logger.append([iters, loss, error])\n",
    "\n",
    "        pbar.set_postfix({\"loss\": f\"{loss:0.8f}\", \"error\": f\"{error:0.8f}\"}, refresh=False)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    # add one last log\n",
    "    u = apply_fn(params, a, b, c, d, e)\n",
    "    error = relative_l2(u, u_gt)\n",
    "    logger.append([iters, loss, error])\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Runtime: {((end-start)/EPOCHS*1000):.2f} ms/iter.\")\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = Path(\"results\")\n",
    "out_folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83644e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = 24\n",
    "\n",
    "for model in [\"CPPINN\", \"TTPINN\", \"TuckerPINN\"]:\n",
    "    model_folder = out_folder / model\n",
    "    model_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    for rank in [6, 8, 12]:\n",
    "        model_folder_rank = model_folder / f\"Rank_{rank:02d}\"\n",
    "        model_folder_rank.mkdir(exist_ok=True)\n",
    "\n",
    "        for run in range(10):\n",
    "            print(f\"Running {model} with rank {rank} and run {run}\")\n",
    "            logs = main(mode=model, NC=points, NI=points, NB=points, NC_TEST=32, SEED=444444 + run, LR=1e-3, EPOCHS=80000, N_LAYERS=4, FEATURES=rank, LOG_ITER=5000)\n",
    "            out_file = model_folder_rank / f\"{model}-Rank_{rank:02d}-Points_{points:02d}-run_{run:02d}.csv\"\n",
    "            pd.DataFrame(logs, columns=[\"Iter\", \"Loss\", \"Error\"]).to_csv(out_file, index=False, float_format=\"%.16f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TD4PINN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
